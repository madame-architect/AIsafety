{"claim_id": "SC-01", "governance_claim": "Pre-deployment capability evaluations used for threshold/gating decisions are valid, reproducible, and robust to evaluation gaming.", "required_evidence": ["Defined capability thresholds + evaluation plans tied to those thresholds", "Anti-gaming protocol: concealed test sets, elicitation best practices, adversarial cue control, replication across labs", "Evidence package: evaluation runs, variance estimates, and failure analysis"], "governance_artifact_ids": ["S2-T3-03", "S2-T3-27", "S2-T3-28", "S2-T3-31", "S2-T3-40", "S2-T3-42", "S2-T3-44", "S2-T3-46", "S2-T3-47", "S2-T3-91", "S2-T3-93"], "candidate_technical_methods": ["T-EGM-002", "T-EGM-003", "T-EGM-004", "T-EGM-005", "T-EGM-006", "T-EGM-034", "T-EGM-035", "T-EGM-016"], "known_failure_modes": ["Evaluation awareness / behavior shift between 'test' and 'deployment' contexts", "Sandbagging and strategic underperformance during evaluation", "Overfitting to benchmark artifacts (Goodharting)", "Distribution shift between eval tasks and real-world deployment"], "test_coverage_gaps": ["Few standardized ways to quantify false-negative rates under adaptive attackers", "Limited cross-institution replication of high-stakes dangerous-capability evals", "Lack of shared concealed test sets with controlled leakage"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-02", "governance_claim": "Evaluations explicitly test sabotage/subversion and monitor evasion in agentic tool-use settings ('AI control' threat model).", "required_evidence": ["Agentic evaluation suites with realistic tool access and organizational constraints", "Red-team protocols that include monitor evasion attempts", "Reporting of monitor coverage, evasion success rates, and mitigations"], "governance_artifact_ids": ["S2-T3-27", "S2-T3-29", "S2-T3-34", "S2-T3-70", "S2-T3-91", "S2-T3-93"], "candidate_technical_methods": ["T-EGM-005", "T-EGM-006", "T-EGM-007", "T-EGM-008", "T-EGM-009", "T-EGM-010"], "known_failure_modes": ["Agents exploit gaps in telemetry visibility (partial logs, missing tool calls)", "Adaptive attackers learn monitor blind spots and exploit reward signals", "Task environments are too stylized, enabling spurious safety conclusions"], "test_coverage_gaps": ["Sparse public benchmarks with end-to-end tool-use + realistic organizational processes", "Little measurement of transfer: evasion strategies across tools/tasks/monitors", "Limited reporting of monitor information access assumptions (what the monitor sees)"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-03", "governance_claim": "Monitoring (including CoT monitoring where used) provides continuous assurance and is treated as one layer in defense-in-depth, with explicit testing for evasion/obfuscation.", "required_evidence": ["Monitor performance curves vs information access (full traces vs filtered excerpts)", "Evasion-resistance tests (obfuscated reasoning, hidden channels, prompt injection)", "Operational thresholds for intervention + incident response playbooks"], "governance_artifact_ids": ["S2-T3-29", "S2-T3-33", "S2-T3-39", "S2-T3-57", "S2-T3-68"], "candidate_technical_methods": ["T-EGM-001", "T-EGM-006", "T-EGM-007", "T-EGM-008", "T-EGM-009"], "known_failure_modes": ["Unfaithful or strategically shaped chain-of-thought (CoT) content", "Monitors suffer from information overload or missing critical events", "Attacker induces monitor distribution shift or exploits summarization filters"], "test_coverage_gaps": ["No consensus on monitorability metrics (faithfulness, completeness, evasion cost)", "Limited studies on monitor incentives (how training choices affect concealment)", "Few open datasets of realistic deployment telemetry for monitors"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-04", "governance_claim": "Incident reporting regimes support comparability and learning: incidents are consistently categorized, tracked over time, and feed back into mitigations.", "required_evidence": ["Standardized incident taxonomy + minimum reporting fields", "Trend reporting and linkage to mitigation updates", "Public/regulated channels for incident submission and triage"], "governance_artifact_ids": ["S2-T3-10", "S2-T3-16", "S2-T3-67", "S2-T3-89", "S2-T3-99"], "candidate_technical_methods": ["T-EGM-016"], "known_failure_modes": ["Under-reporting (reputational incentives) and inconsistent categorization", "Selection bias toward visible, high-profile incidents", "Non-actionable reports that don't connect to mitigations"], "test_coverage_gaps": ["Weak verification mechanisms for self-reported incidents", "Limited linkage between incident classes and specific evaluation tests", "Insufficient longitudinal public datasets for incident trend modeling"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-05", "governance_claim": "Transparency reporting frameworks (e.g., HAIP) enable cross-organization comparability and verification of safety practices.", "required_evidence": ["Machine-readable disclosure fields and evidence tags (artifact type, assurance function, domain, verification strength)", "Provenance for each claim (primary evidence vs self-report)", "Defined verification hooks (third-party audit, replication, regulator access)"], "governance_artifact_ids": ["S2-T3-10", "S2-T3-11", "S2-T3-12", "S2-T3-14", "S2-T3-15", "S2-T3-89"], "candidate_technical_methods": ["T-EGM-016"], "known_failure_modes": ["Goodharting disclosures (policies that look good but don't bind behavior)", "Ambiguous claims that cannot be independently checked", "Inconsistent scope definitions across reporters"], "test_coverage_gaps": ["Few shared schemas for evidence attachments (system cards, eval logs, audits)", "Limited mechanisms to prevent cherry-picked reporting", "Lack of standard adversarial tests linked to disclosed mitigations"], "notes": "This governance-atlas package provides the requested evidence tags for Tier Three artifacts; extending to developer submissions is a next step.", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-06", "governance_claim": "Open-weight and fine-tuning ecosystems include integrity checks: backdoors/poisoning are detectable, and post-training safety cannot be trivially bypassed by downstream tuning.", "required_evidence": ["Supply-chain scanning for backdoors and poisoning indicators", "Robustness tests under downstream fine-tuning (malicious and benign)", "Provenance controls and reproducible training pipelines"], "governance_artifact_ids": ["S2-T3-23", "S2-T3-55", "S2-T3-54", "S2-T3-61", "S2-T3-62", "S2-T3-63", "S2-T3-64"], "candidate_technical_methods": ["T-EGM-019", "T-EGM-026", "T-EGM-027", "T-EGM-028", "T-EGM-029", "T-EGM-030", "T-EGM-031", "T-EGM-033", "T-EGM-045"], "known_failure_modes": ["Backdoors persist through safety training and updates", "Backdoors activate only after downstream fine-tuning (latent triggers)", "Poisoning succeeds with small absolute poison counts; dilution assumptions fail"], "test_coverage_gaps": ["Limited scalable certification pipelines for open-weight releases", "Sparse benchmarks for 'latent until fine-tuned' supply-chain attacks", "Need standardized reporting of scanning coverage and false negatives"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-07", "governance_claim": "Safety documentation and assurance cases are auditable and versioned: claims trace to evidence artifacts (system cards, TEVV results, change logs).", "required_evidence": ["Documentation standard specifying minimum fields (training, evals, mitigations, known limitations)", "TEVV procedures + audit trails for evaluation runs", "Versioned change logs and regression testing for updates"], "governance_artifact_ids": ["S2-T3-01", "S2-T3-03", "S2-T3-04", "S2-T3-20", "S2-T3-47", "S2-T3-84"], "candidate_technical_methods": ["T-EGM-016", "T-EGM-021", "T-EGM-023", "T-EGM-040"], "known_failure_modes": ["Documentation drift: outdated claims after model updates", "False assurance from weak evaluation metrics (e.g., unlearning metrics)", "Non-reproducible evaluation pipelines"], "test_coverage_gaps": ["Few public 'assurance case' exemplars with end-to-end traceability", "Missing standard for evidence packages (what to attach, in what format)", "No common metric for regression in safety properties across versions"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-08", "governance_claim": "Dangerous-capability evaluations (cyber, bio, persuasion, etc.) are standardized, replicated, and linked to mitigations and deployment/release decisions.", "required_evidence": ["Domain-specific eval suites with clearly defined threat models", "Replication across institutions + shared baselines", "Mitigation-aware evaluation (safety under mitigations, not raw capability only)"], "governance_artifact_ids": ["S2-T3-27", "S2-T3-33", "S2-T3-34", "S2-T3-50", "S2-T3-46", "S2-T3-47", "S2-T3-85"], "candidate_technical_methods": ["T-EGM-025", "T-EGM-041", "T-EGM-043", "T-EGM-044", "T-EGM-045"], "known_failure_modes": ["Benchmarks lag real attackers; attack evolution invalidates results", "Capability uplift depends on tool scaffolding and access assumptions", "Comparisons across models confounded by leakage/contamination"], "test_coverage_gaps": ["Sparse public bio/CBRN evaluation methods with safe test harnesses", "Few open protocols for measuring persuasive harm at scale", "Limited evidence linking eval scores to concrete risk thresholds"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-09", "governance_claim": "Compute governance and diffusion controls are enforceable, with measurable obligations and compliance evidence.", "required_evidence": ["Clear definitions of covered systems/compute thresholds", "Compliance plans and reporting pipelines for affected agencies/orgs", "Auditability of compute and model-weight movement"], "governance_artifact_ids": ["S2-T3-22", "S2-T3-23", "S2-T3-24", "S2-T3-61", "S2-T3-62", "S2-T3-63", "S2-T3-64"], "candidate_technical_methods": ["T-EGM-040", "T-EGM-042", "T-EGM-045"], "known_failure_modes": ["Evasion via distributed/obfuscated compute usage", "Ambiguity in threshold definitions or measurement", "Regulatory lag vs rapid model iteration"], "test_coverage_gaps": ["Little public technical work on auditable compute measurement for governance", "Weak linkage between compute thresholds and demonstrated risk thresholds", "Need validated methods to estimate 'capability per compute' under different training recipes"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
{"claim_id": "SC-10", "governance_claim": "Frontier safety frameworks specify triggers, mitigations, and verification pathways that can be translated into testable safety-case arguments.", "required_evidence": ["Explicit claim structure: hazard → claim → evidence → argument → residual risk", "Defined verification: independent evals, red-teams, audits, and post-deployment monitoring", "Publication of failure modes and test coverage gaps"], "governance_artifact_ids": ["S2-T3-28", "S2-T3-29", "S2-T3-32", "S2-T3-37", "S2-T3-39", "S2-T3-40", "S2-T3-42", "S2-T3-44", "S2-T3-92"], "candidate_technical_methods": ["T-EGM-005", "T-EGM-008", "T-EGM-016", "T-EGM-040", "T-EGM-041"], "known_failure_modes": ["Frameworks become checklists without binding evidence requirements", "Verification relies on self-report without third-party replication", "Mitigations do not generalize across models/tasks"], "test_coverage_gaps": ["Need shared safety-case templates with measurable acceptance criteria", "Need public corpora of 'known failure modes' linked to standardized tests", "Need evaluation governance to manage leakage and adversarial adaptation"], "notes": "", "atlas_version": "v1.0.0-2026-02-14", "schema_version": "safety-case-atlas-schema-1.0.0"}
